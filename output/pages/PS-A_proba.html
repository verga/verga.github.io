<!DOCTYPE html>
<html lang="en">

  <head>
    <!-- Required meta tags always come first -->
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Applications: Probability | Random physics
</title>
    <link rel="canonical" href="/pages/PS-A_proba.html">

    <link rel="stylesheet" href="/theme/css/bootstrapr.min.css">
    <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
    <link rel="stylesheet" href="/theme/css/pygments/autumn.min.css">
    <link rel="stylesheet" href="/theme/css/style.css">

    <link rel="icon" type="image/png" href="/extras/rphys.png" sizes="64x64">

<meta name="description" content="Brief introduction to probability theory; solution of selected exercises.">

  </head>

  <body>
    <header class="header">
      <div class="container">
        <div class="row">
          <div class="col-sm-12">
            <h1 class="title"><a href="/">Random physics</a></h1>
            <p class="text-muted">Alberto Verga, research notebook</p>
              <ul class="list-inline">
                  <li class="list-inline-item"><a href="/">Blog</a></li>
                      <li class="list-inline-item text-muted">|</li>
                      <li class="list-inline-item"><a href="/pages/about.html">About</a></li>
                      <li class="list-inline-item"><a href="/pages/lectures.html">Lectures</a></li>
              </ul>
          </div>
        </div>
      </div>
    </header>

    <div class="main">
      <div class="container">

<article class="article">
  <div class="content">
    <p><span class="math">\(\newcommand{\I}{\mathrm{i}} 
\newcommand{\E}{\mathrm{e}} 
\newcommand{\D}{\mathop{}\!\mathrm{d}} 
\newcommand{\bra}[1]{\langle{#1}|}
\newcommand{\ket}[1]{|{#1}\rangle}
\newcommand{\braket}[1]{\langle{#1}\rangle}
\newcommand{\bbraket}[1]{\langle\!\langle{#1}\rangle\!\rangle}&nbsp;\newcommand{\Tr}{\mathrm{Tr}}\)</span></p>
<blockquote>
<p><a href="/pages/PS-index.html">Lectures</a> on statistical&nbsp;mechanics.</p>
</blockquote>
<h1>Probability<sup id="fnref:SH"><a class="footnote-ref" href="#fn:SH">1</a></sup></h1>
<h2>Theory</h2>
<p>A probability space is defined by a set <span class="math">\(E\)</span>, the events or outcomes of an experiment, which are the subsets <span class="math">\(E_n \subset E\)</span> (<span class="math">\(n=1,2,\ldots\)</span>), and a measure <span class="math">\(P(E_n)\)</span> satisfying the following <em>Kolmogorov</em>&nbsp;axioms:</p>
<ul>
<li><span class="math">\(P( \emptyset ) =&nbsp;0\)</span></li>
<li><span class="math">\(P(E) =&nbsp;1\)</span></li>
<li>if <span class="math">\(n \ne m\)</span> and <span class="math">\(E_n \cap E_m = \emptyset\)</span>, <span class="math">\(P(\cup_n E_n) = \sum_n P(E_n)\)</span>.</li>
</ul>
<p>As a consequence on&nbsp;finds,
</p>
<div class="math">$$P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)\,.$$</div>
<p>The phenomenological measure of the event <span class="math">\(E_n\)</span> is given by its <em>frequency</em>: if <span class="math">\(E_n\)</span> is the outcome of some experiment, repeating the experiment <span class="math">\(N\)</span> times, the frequency&nbsp;is,
</p>
<div class="math">$$\nu_N(E_n) = \frac{\# E_n}{N}\,,$$</div>
<p>
where <span class="math">\(\# E_n\)</span> is the number of occurrences of <span class="math">\(E_n\)</span> in the <span class="math">\(N\)</span> experiments.&nbsp;Therefore,
</p>
<div class="math">$$P(E_n) = \lim_{N \rightarrow \infty} \nu_N(E_n)\,.$$</div>
<p>An event can be associated with a <em>random variable</em> <span class="math">\(X\)</span>, which takes the numerical values <span class="math">\(x\)</span>, in the integers if discrete, with probability <span class="math">\(p(x)\)</span>:
</p>
<div class="math">$$0\le p(x) \le 1\,,\quad \sum_{x \in X} p(x) =1\,.$$</div>
<p>
The expected value of <span class="math">\(X\)</span> is given by the weighted&nbsp;sum,
</p>
<div class="math">$$\mathrm{E}[X] = \sum_{x \in X} x p(x)\,, \quad \braket{x} = \mathrm{E}[X]$$</div>
<p>
And, more generally, the moments of degree <span class="math">\(n\)</span>&nbsp;are,
</p>
<div class="math">$$\braket{ x^n } = \sum_{x \in X} x^n p(x)\,.$$</div>
<p>
The variance and the standard deviation are defined by the second&nbsp;moment:
</p>
<div class="math">$$\braket{\Delta x^2} = \braket{x^2} - \braket{x}^2$$</div>
<p>
and,
</p>
<div class="math">$$\sigma = \mathrm{std} (x) = \sqrt{ \braket{\Delta x^2}}\,.$$</div>
<p>
In the case of a continuous random variable <span class="math">\(X = x \in \mathbb{R}\)</span>, the above formulas generalize straightforwardly. One defines, instead of the discrete distribution <span class="math">\(p\)</span>, a continuous distribution function <span class="math">\(F(x) = P(X&lt;x)\)</span> which gives the probability of the event <span class="math">\(\{X&lt;x\}\)</span>. Its derivative is the probability <em>density</em>&nbsp;function,
</p>
<div class="math">$$f(x)\ge0, \quad \int_{-\infty}^\infty \D x \, f(x) =1$$</div>
<p>
with,
</p>
<div class="math">$$P(X&lt;x) = F(x) = \int_{-\infty} ^x \D x \, f(x)\,,\quad \frac{\D F(x)}{\D x}=f(x)$$</div>
<p>
Therefore,
</p>
<div class="math">$$\braket{x^n} = \int_{-\infty}^\infty \D x\, x^n \, f(x)\,.$$</div>
<p>The joint probability of a set of random variables <span class="math">\(\boldsymbol X = (X_1,X_2,\ldots)\)</span> is <span class="math">\(P(\boldsymbol X)\)</span>. Two variables <span class="math">\(X,Y\)</span> are <em>independent</em> if their joint probability&nbsp;factorizes:
</p>
<div class="math">$$P(X,Y) = P(X) P(Y)\,,$$</div>
<p>
or
</p>
<div class="math">$$f(x,y) = f(x) f(y)\,,$$</div>
<p>
in the continuous&nbsp;case.</p>
<p>It is convenient to introduce the <em>characteristic function</em> <span class="math">\(g\)</span> of the density distribution <span class="math">\(f\)</span>:
</p>
<div class="math">$$g_X(k) = \int_{-\infty}^\infty \D x \, \E^{-\I kx}f(x)\,.$$</div>
<p>
The characteristic function of a sum of random variables is the product of the characteristic&nbsp;functions:
</p>
<div class="math">$$Y = \sum_n X_n \; \Rightarrow \; g_Y(k) = \prod_n g_{X_n}(k)\,.$$</div>
<p>
The logarithm of the characteristic function is the <em>cumulant</em> generating&nbsp;function.</p>
<p>The characteristic function can also be defined by the expected&nbsp;value:
</p>
<div class="math">$$g_X(k) = \braket{\E^{-\I k x}} = \sum_n \frac{(-\I k)^n}{n!} \braket{x^n}\,,$$</div>
<p>
which shows that the coefficients of its series expansion in <span class="math">\(k\)</span> are the <em>moments</em> <span class="math">\(\braket{x^n}\)</span>. The moments may be computed through the&nbsp;derivatives:
</p>
<div class="math">$$\braket{x^n} =  \I^n \left( \frac{\partial }{\partial k} \right)^n \left. \braket{\E^{-\I k x}} \right|_{k=0}\,.$$</div>
<h3>Examples: counting&nbsp;configurations</h3>
<blockquote>
<table style="width:300px;">
<caption> Select \(n\) objects out of a total of \(N\) </caption>
<tr>
<th>Order</th>
<th>Repetition</th>
<th>   Number   </th>
</tr>
<tr>
<td>yes</td>
<td>yes</td>
<td>\(N^n\)  </td>
</tr>
<tr>
<td>yes</td>
<td>no</td>
<td>\(P^N_n=\frac{N!}{(N-n)!}\)</td>
</tr>
<tr>
<td>no</td>
<td>yes</td>
<td>\(\binom{N+n-1}{n}\)  </td>
</tr>
<tr>
<td>no</td>
<td>no</td>
<td>\(\binom{N}{n}=\frac{N!}{n!(N-n)!}\)</td>
</tr>
</table>
</blockquote>
<p>As an example we take <span class="math">\(N\)</span> indistinguishable particles (objects) which can be in <span class="math">\(S\)</span> different states <span class="math">\(s=1,\ldots,S\)</span> (box), such&nbsp;that
</p>
<div class="math">$$N = \sum_{s=1}^S n_s\,, \quad n_s = 0, 1, \ldots$$</div>
<p>
where <span class="math">\(n_s\)</span> is the number of particles in state <span class="math">\(s\)</span> (this number can be zero). The number <span class="math">\(\Omega\)</span> of sequences <span class="math">\((n_1,\ldots, n_S)\)</span>, representing microscopic states of <span class="math">\(N\)</span> particles, is equivalent to the number of arrangements of <span class="math">\(N\)</span> objects in <span class="math">\(S\)</span> boxes with&nbsp;repetition:
</p>
<div class="math">$$\Omega = \binom{S+N-1}{N}$$</div>
<p>
The probability of an elementary event is then <span class="math">\(P=1/\Omega\)</span>, the bose-einstein distribution in the microcanonical ensemble. The fermi-dirac case is obtained by restricting the number of particles to the set <span class="math">\(n_s=0,1\)</span>, so we have no more than one particle in a given state. In this case <span class="math">\(S&gt;N\)</span>, <span class="math">\(\Omega\)</span> is the number of combinations of <span class="math">\(N\)</span> objects in <span class="math">\(S\)</span>&nbsp;boxes,
</p>
<div class="math">$$\Omega = \binom{S}{N}\,.$$</div>
<p>Other example is given by the Bernoulli process, in which we are interested in the number <span class="math">\(n\)</span> of successes in <span class="math">\(N\)</span> trials, if the probability of success is <span class="math">\(p\)</span>. The probability of exactly <span class="math">\(n\)</span> occurrences of success among <span class="math">\(N\)</span> events&nbsp;is:
</p>
<div class="math">$$P_N(n) = \binom{N}{n} p^n \, (1-p)^{N-n}\,.$$</div>
<p>
This formula can be applied to the random walk, choosing <span class="math">\(p=1/2\)</span> the probability to go to the left or to the right, <span class="math">\(N=t\)</span> the number of time steps, and <span class="math">\(x=n - (N-n) = 2n -N\)</span> the position of the walker after <span class="math">\(n\)</span> steps to the right and <span class="math">\(N-n\)</span> steps to the&nbsp;left:
</p>
<div class="math">$$p(x,t) = \binom{t}{\frac{x+t}{2}} \frac{1}{2^t}\,.$$</div>
<p>
The long time limit gives a gaussian distribution. Indeed, using the stirling&nbsp;formula:
</p>
<div class="math">$$\ln N! \approx N \ln N - N\,,$$</div>
<p>
one&nbsp;gets,
</p>
<div class="math">$$\ln p(x,t) \approx -t \ln 2 + t \ln t -\frac{t+x}{2} \ln \frac{t+x}{2} + \frac{t+x}{2} - \frac{t-x}{2} \ln \frac{t-x}{2} + \frac{t-x}{2}\,,$$</div>
<p>
and substituting the expansion (<span class="math">\(t \gg x\)</span>),
</p>
<div class="math">$$\ln \frac{t \pm x}{2} = \ln \frac{t}{2} \pm \frac{x}{t}\,,$$</div>
<div class="math">$$\ln p(x,t) \approx t \ln \frac{t}{2} - \frac{t+x}{2} \left(\ln \frac{t}{2} + \frac{x}{t}\right) - \frac{t-x}{2} \left(\ln \frac{t}{2} - \frac{x}{t}\right) $$</div>
<p>
which simplifies&nbsp;to,
</p>
<div class="math">$$\ln p(x,t) \approx - \frac{x^2}{t}\,.$$</div>
<p>
After normalization one obtains the normal distribution <span class="math">\(\mathcal{N}(0,1/\sqrt{2})\)</span> of zero mean and variance <span class="math">\(1/2\)</span>:
</p>
<div class="math">$$p(x,t) = \frac{\E^{-x^2/t}}{\sqrt{\pi t}}\,.$$</div>
<p>
This result is compatible with the central limit&nbsp;theorem.</p>
<h2>The central limit&nbsp;theorem</h2>
<p>The theorem of the <em>central limit</em> states that the sum of a large number of random variables is normally&nbsp;distributed:
</p>
<div class="math">$$Z_N = \frac{1}{\sqrt{N}\sigma} \left(\sum_n^N X_n-Nm\right) \;\rightarrow\;  \mathcal{N}(0,1)$$</div>
<p>
where <span class="math">\(X_n\)</span> are independent random variables with <span class="math">\(\braket{X} = m\)</span> and <span class="math">\(\mathrm{std}\,X = \sigma\)</span>.</p>
<p>It is simpler to work with the normalized variables <span class="math">\(Y_n=(X_n-m)/\sigma\)</span> of mean zero and unit variance. The characteristic function of <span class="math">\(Z\)</span>&nbsp;is,
</p>
<div class="math">$$g_Z(k) = g_Y(k/\sqrt{N})^N$$</div>
<p>
where we used the identity <span class="math">\(g_{X/a}(k) = g_X(k/a)\)</span>. In the large <span class="math">\(N\)</span> limit, the argument of <span class="math">\(g\)</span> is small and <span class="math">\(g\)</span> can be expanded in a taylor&nbsp;series:
</p>
<div class="math">$$g_Y(k/\sqrt{N}) = 1 - \I \braket{y} \frac{k}{\sqrt{N}} + [-\braket{y^2} + R(k/\sqrt{N})]  \frac{k^2}{2N}$$</div>
<p>
where the error term vanishes faster than <span class="math">\(k^2\)</span>. Applying this formula to <span class="math">\(g_Y\)</span>, one&nbsp;obtains,
</p>
<div class="math">$$g_Z(k) = \left[ 1 - \big(1- R(k/\sqrt{N}) \big)  \frac{k^2}{2N}  \right]^N$$</div>
<p>
Taking now the logarithm and using <span class="math">\(\ln(1 \pm x) \approx \pm x\)</span>, we&nbsp;find,
</p>
<div class="math">$$\lim_{N\rightarrow\infty} \ln g_Z(k) = - \frac{k^2}{2} + \lim_{N\rightarrow\infty} R(k/\sqrt{N}) \frac{k^2}{2}= - \frac{k^2}{2}\,,$$</div>
<p>
which leads&nbsp;to
</p>
<div class="math">$$g_Z(k) = \E^{- \frac{k^2}{2}}\,,$$</div>
<p>
essentially the statement of the theorem. The gaussian distribution of <span class="math">\(Z\)</span> is obtained using the inverse&nbsp;transform,
</p>
<div class="math">$$f(z) = \frac{1}{2\pi}\int_{-\infty}^\infty \D k \, \E^{\I kz}g_Z(k) = \frac{\E^{-z^2/2}}{\sqrt{2\pi}}\,.$$</div>
<h3>Example: computing the characteristic&nbsp;function</h3>
<p>The normal distribution is a probability law of the form <span class="math">\(f(x) \sim \E^{-x^2/2}\)</span>, the gaussian bell. Under an affine transformation <span class="math">\(x = (X-m)/\sigma\)</span>, a translation invariant distribution function (like the normal distribution), the distribution <span class="math">\(F(X)\)</span> changes&nbsp;as:
</p>
<div class="math">$$1 = \int_{-\infty}^\infty \D X \, F(X) =  \int_{-\infty}^\infty \D (\sigma x + m) \, F(X(x)) = \sigma \int_{-\infty}^\infty \D x \frac{1}{\sigma} f(x)\,,$$</div>
<p>
or,
</p>
<div class="math">$$F(X) = \frac{1}{\sigma} f(x), \quad x = \frac{X-m}{\sigma}\,.$$</div>
<p>
Therefore it is enough to&nbsp;consider,
</p>
<div class="math">$$f(x) = \frac{\E^{-x^2/2}}{\sqrt{2\pi}} \sim \mathcal{N}(0,1)\,,$$</div>
<p>
and eventually, to obtain <span class="math">\(F(X) \sim \mathcal{N}(m,\sigma)\)</span>, we make the variable&nbsp;change
</p>
<div class="math">$$x \rightarrow x(X) = \frac{X-m}{\sigma}, \quad f(x) \rightarrow F(X) = \frac{1}{\sigma}f(x(X)) = \frac{\E^{-(X-m)^2/2\sigma^2}}{\sqrt{2\pi \sigma^2}}\,.$$</div>
<p>The characteristic function&nbsp;is,
</p>
<div class="math">\begin{align*}g(k) &amp;= \int_{-\infty}^\infty \frac{\D x}{\sqrt{2\pi}} \E^{-\I k x - x^2/2} \\ &amp; = \int_{-\infty}^\infty \frac{\D x}{\sqrt{2\pi}} \exp\left[-\frac{1}{2}(x^2 - 2\I k x - k^2) -\frac{k^2}{2} \right] \\ &amp; = \E^{-k^2/2}\, \int_{-\infty}^\infty \frac{\D x}{\sqrt{2\pi}} \E^{-(x-\I k )^2/2} \\ &amp; = \E^{-k^2/2}\,.\end{align*}</div>
<p>
Note that the cumulant generating function is at most, a second degree polynomial in <span class="math">\(k\)</span>; this immediately implies that the only non vanishing cumulants are the mean and the variance. The characteristic function can easily expanded in series of <span class="math">\(k\)</span>:
</p>
<div class="math">$$g(k) = \sum_{n=0}^\infty \frac{(-1)^n}{2^n} \frac{k^{2n}}{n!}\,,$$</div>
<p>
which gives the even&nbsp;moments,
</p>
<div class="math">$$\braket{x^{2n}} = \frac{1}{2^n} \frac{(2n)!}{n!} \equiv (2n-1)!!$$</div>
<p>
and, as anticipated, the odd moments are&nbsp;zero.</p>
<p>If the probability density <span class="math">\(f(x)\)</span> do not decrease fast enough when <span class="math">\(x\rightarrow\infty\)</span>, the moments might not exist; this is the case of the cauchy distribution <span class="math">\(f(x) \sim 1/(1+x^2)\)</span>. We start by finding the normalization&nbsp;constant:
</p>
<div class="math">$$\int_{-\infty}^\infty \frac{\D x}{1+x^2} = \pi$$</div>
<p>
hence,
</p>
<div class="math">$$f(x) = \frac{1}{\pi} \frac{1}{1 + x^2}\,.$$</div>
<p>
The first moment is zero by parity, but the second&nbsp;moment,
</p>
<div class="math">$$ \frac{1}{\pi}\int_{-\infty}^\infty \frac{\D x\,x^2}{1+x^2} = \text{ divergent}\,.$$</div>
<p>
The characteristic function <span class="math">\(g(k)\)</span> is in fact a non&nbsp;analytic:
</p>
<div class="math">$$g(k) = \E^{-|k|}\,,$$</div>
<p>
it depends on the absolute value of <span class="math">\(k\)</span>, which has no derivative at <span class="math">\(k=0\)</span> (where we compute the moments). Using the residues theorem we can compute the fourier&nbsp;integral,
</p>
<div class="math">$$g(k) = \frac{1}{\pi}\int_{-\infty}^\infty \frac{\D x\, \E^{-\I kx}}{1+x^2}\,,$$</div>
<p>
indeed,
</p>
<div class="math">$$\frac{1}{1+x^2} = \frac{1}{2\I} \left( \frac{1}{x-\I} - \frac{1}{x + \I}   \right)\,,$$</div>
<p>
the first term, the <span class="math">\(x=\I\)</span> residue, gives <span class="math">\(\E^k\)</span> (with <span class="math">\(k&lt;0\)</span>), closing the contour on the upper complex <span class="math">\(x\)</span> plane, and the second one, <span class="math">\(x=-\I\)</span>, gives <span class="math">\(\E^{-k}\)</span> (with <span class="math">\(k&gt;0\)</span>), and closing the contour on the lower complex&nbsp;plane.</p>
<h2>Probability theory of&nbsp;cumulants</h2>
<p>The cumulant generating function is then given&nbsp;by,
</p>
<div class="math">$$c(k) = \ln g(k) = \sum_n \bbraket{x^n} \frac{(-\I k)^n}{n!}$$</div>
<p>
where the second expression <em>defines</em> the cumulant <span class="math">\(\bbraket{x^n}\)</span>. Comparing the two expressions, at each order one may compute the cumulant in terms of the&nbsp;moments.</p>
<p>As we saw before, if <span class="math">\(X_1\)</span> and <span class="math">\(X_2\)</span> are two independent random variables,&nbsp;then,
</p>
<div class="math">$$\braket{\E^{\I k( x_1 + x_2)}} = \braket{\E^{\I k x_1}} \braket{\E^{\I k x_2}}\,.$$</div>
<p>
Therefore, the cumulant function <span class="math">\(c_X(k) = \ln g_X(k)\)</span>, possesses the &#8220;extensivity&#8221;&nbsp;property,
</p>
<div class="math">$$\ln\braket{\E^{-\I k (x_1 + x_2)}} = \ln\braket{\E^{-\I k x_1}} + \ln\braket{\E^{-\I k x_2}} \,.$$</div>
<p>
It is the generating function of the <em>cumulants</em>:
</p>
<div class="math">$$\bbraket{x^n} = \left( \frac{-1}{\I} \frac{\partial }{\partial k} \right)^n \left. c_X(k) \right|_{k=0}\,.$$</div>
<p>
An explicit calculation leads to the following relations between the cumulants <span class="math">\(\bbraket{x^n}\)</span> and the moments <span class="math">\(\braket{x^n}\)</span> <strong style="color:DarkSlateBlue; background-color:LightGray;"><span class="caps">EX</span></strong>,
</p>
<div class="math">\begin{align*}
\bbraket{x} &amp; = \braket{x}\\
\bbraket{x^2} &amp; = \braket{x^2} - \braket{x}^2\\
\bbraket{x^3} &amp; = \braket{x^3} - 3 \braket{x^2} \braket{x} + 2 \braket{x}^3\\
\bbraket{x^4} &amp; = \braket{x^4} - 3 \braket{x^2}^2 - 4 \braket{x^3} \braket{x} + 12 \braket{x^2}\braket{x}^2 - 6 \braket{x}^4
\end{align*}</div>
<p>
The general form of the relations between cumulants and moments contains the <a href="https://www.encyclopediaofmath.org/index.php/Bell_polynomial">Bell polynomials</a>:
</p>
<div class="math">$$\braket{x^n} = \sum_{k=1}^n B_{n,k}(\bbraket{x},\ldots,\bbraket{x^{n-k+1}})$$</div>
<p>
and the inverse&nbsp;formula,
</p>
<div class="math">$$\bbraket{x^n} = \sum_{k=1}^n (-1)^{k-1} (k-1)! B_{n,k}(\braket{x},\ldots,\braket{x^{n-k+1}})$$</div>
<p>
The Bell polynomials encode the information about the partition of <span class="math">\(n\)</span> into <span class="math">\(k\)</span> blocks; for instance, <span class="math">\(B_{n,k}(1,\ldots,1)\)</span> is the number of ways to partition a set <span class="math">\(\{1,\ldots,n\}\)</span> into <span class="math">\(k\)</span> subsets. Their explicit form&nbsp;is,
</p>
<div class="math">$$B_{n,k}(x_1,\ldots x_{K}) = \sum_{s_1,\ldots,s_K} \delta\left(\sum_{l=1}^K s_l,k\right) \delta\left(\sum_{l=1}^K ls_l,n\right) n! \prod_{l=1}^K \frac{x_l^{s_l}}{s_l! l!^{s_l}}$$</div>
<p>
(<span class="math">\(K=n-k+1\)</span>) where the two Kronecker deltas account for the constraints on the indices <span class="math">\(s_l\)</span>. In this formula <span class="math">\(l\)</span> is the number of elements in each group (the size of the <em>cluster</em>), <span class="math">\(s_l\)</span> is the number of clusters of size <span class="math">\(l\)</span>. We note&nbsp;that, 
</p>
<div class="math">$$\frac{n!}{\prod_l l!^{s_l}}$$</div>
<p>
counts the number of combinations to pick the <span class="math">\(l\)</span>-clusters from the set of <span class="math">\(n\)</span> elements; and the factors <span class="math">\(s_l!\)</span> are the number of permutations of the cluster <span class="math">\(l\)</span>, giving the same partition (the same monomial) <strong style="color:DarkSlateBlue; background-color:LightGray;"><span class="caps">EX</span></strong>.</p>
<p>As an example, the cumulant generating function of the gaussian distribution of mean <span class="math">\(m\)</span> and variance <span class="math">\(\sigma^2\)</span> is <span class="math">\(\ln\braket{\E^{-\I k x}} = \I m k - \sigma^2 k^2/2\)</span>, giving <span class="math">\(\bbraket{x^n} =0\)</span> for <span class="math">\(n&gt;2\)</span> <strong style="color:DarkSlateBlue; background-color:LightGray;"><span class="caps">EX</span></strong>.</p>
<p>Generalization to <span class="math">\(N\)</span> variables is straightforward. The generating functions&nbsp;are:
</p>
<div class="math">$$\braket{x_1 \ldots x_N} = \left( \I^N \frac{\partial^N }{\partial k_1 \ldots \partial k_N} \right) \left. \braket{\E^{-\I \boldsymbol k \cdot \boldsymbol x}} \right|_{k=0}\,.$$</div>
<p>
where <span class="math">\(\boldsymbol x = (x_1,\ldots,x_N\)</span> and <span class="math">\(\boldsymbol k = (k_1,\ldots,k_N)\)</span>. Similarly for the&nbsp;cumulants,
</p>
<div class="math">$$\bbraket{x_1 \ldots x_N} = \left( \I^N \frac{\partial^N }{\partial k_1 \ldots \partial k_N} \right) \left. \ln \braket{\E^{-\I \boldsymbol k \cdot \boldsymbol x}} \right|_{k=0}\,.$$</div>
<p>
For example, the third order cumulant&nbsp;is:
</p>
<div class="math">$$\bbraket{x_1 x_2 x_3} = \braket{x_1 x_2 x_3} - \braket{x_1 x_2} \braket{x_3} - \braket{x_1 x_3} \braket{x_2} - \braket{x_2 x_3} \braket{x_1} + 2\braket{x_1} \braket{x_2} \braket{x_3} \,,$$</div>
<p>
which reduces to the previous formula if <span class="math">\(x_1=x_2=x_3\)</span>. It can be also written as (using only the indices to simplify the&nbsp;notation),
</p>
<div class="math">$$\{(123)-(1)(2)(3)\} - \{ [(12) - (1)(2)](3) + [(13)- (1)(3)](2) + [(23) - (2)(3) ](1) \}\,,$$</div>
<p>
making evident that if one of the moments factorizes the corresponding cumulant, vanishes. We can also invert the previous formulas to find the moments as a combination of cumulants <strong style="color:DarkSlateBlue; background-color:LightGray;"><span class="caps">EX</span></strong>:
</p>
<div class="math">$$\braket{x_1x_2x_3} = \bbraket{x_1x_2x_3} + \bbraket{x_1x_2}\bbraket{x_3} + \bbraket{x_1x_3}\bbraket{x_2} + \bbraket{x_2x_3}\bbraket{x_1} + \bbraket{x_1}\bbraket{x_2}\bbraket{x_3}\,.$$</div>
<p>
Knowledge of the cumulants of a random variable is enough to reconstruct the probability distribution. It is interesting to note that the above general formula of the multivariate moments can be written in a combinatorial form (see the discussion on the one random variable case) <strong style="color:DarkSlateBlue; background-color:LightGray;"><span class="caps">EX</span></strong><sup id="fnref2:SH"><a class="footnote-ref" href="#fn:SH">1</a></sup>:
</p>
<div class="math">$$\braket{x_1\ldots x_n} = \sum_{\mathcal{P}} \prod_{s\in \mathcal{P}} \bbraket{x_s}$$</div>
<p>
where, </p>
<ul>
<li><span class="math">\(\mathcal{P}\)</span> is one of the possible partitions of the set <span class="math">\(\{1,\ldots,n\}\)</span> in <span class="math">\(|\mathcal{P}|\)</span> blocks (we denote <span class="math">\(|\mathcal{S}|\)</span> the number of elements, cardinal, of the set <span class="math">\(\mathcal{S}\)</span>),</li>
<li><span class="math">\(s \in \mathcal{P}\)</span> is an element of the list of blocks belonging to one partition <span class="math">\(\mathcal{P}\)</span>,</li>
<li><span class="math">\(x_{s}\)</span> is a product of <span class="math">\(|s|\)</span> variables indexed by the block <span class="math">\(s\)</span>.</li>
</ul>
<p>The inverse formula&nbsp;reads,
</p>
<div class="math">$$\bbraket{x_1\ldots x_n} = \sum_{\mathcal{P}} (-1)^{|\mathcal{P|}-1}|\mathcal{P}-1|! \prod_{s\in \mathcal{P}} \braket{x_s}$$</div>
<p>
An important consequence of this statement is that the cumulants are <em>irreducible</em>: the cumulant involving random <em>independent</em> variables is zero. In the language of graphs we introduce below, one refers to this property as that the cumulants correspond to <em>connected</em> graphs, graphs which cannot be decomposed into subgraphs by cutting an&nbsp;edge.</p>
<p>Therefore, the cumulants measure the non trivial, irreducible, correlations of a random variable, i.e. correlations that cannot be expressed in terms of lower order&nbsp;cumulants.</p>
<h2>Selected solutions to the&nbsp;exercises</h2>
<h3>1</h3>
<p>The Rayleigh distribution&nbsp;is
</p>
<div class="math">$$f(x) = 2 x \E^{-x^2}$$</div>
<p>
and the corresponding generating function&nbsp;is
</p>
<div class="math">$$g_X(k) = \E^{-k^2/2} - \frac{\I \sqrt{\pi}}{2} k \E^{-k^2/4} \mathrm{erfc}\left( \frac{\I k}{2} \right)$$</div>
<p>
where
</p>
<div class="math">$$\mathrm{erfc}(x) = \frac{2}{\sqrt{\pi}} \int_x^\infty \D t \, \E^{-t^2}$$</div>
<p>
is the complementary <a href="https://en.wikipedia.org/wiki/Error_function">error function</a>.</p>
<p>The moments can be calculated using the derivatives of <span class="math">\(g_X\)</span> or&nbsp;directly:
</p>
<div class="math">$$\braket{x^n} = 2 \int_0^\infty \D x \, x^{n+1} \E^{-x^2} = \Gamma\left(\frac{n}{2} + 1\right)$$</div>
<p>
where
</p>
<div class="math">$$\Gamma(x) = \int_0^\infty \D t \, t^{x-1} \E^{-t}$$</div>
<p>
is the Euler <a href="https://en.wikipedia.org/wiki/Gamma_function">gamma function</a>.</p>
<h3>4</h3>
<p>Information and entropy. Use the maximum entropy principle to find the distribution <span class="math">\(p(v)\)</span> of the velocity <span class="math">\(v\)</span> of one particle subject to the constraints: (V) <span class="math">\(\braket{|v|} = c\)</span>, (E) <span class="math">\(\braket{mv^2/2} = mc^2/2\)</span>. Show that the energy constraint (E) contains more information than the mean velocity constraint&nbsp;(V),
</p>
<div class="math">$$I_\text{E} - I_\text{V} &gt; 0$$</div>
<p>
compute this information difference in&nbsp;bits.</p>
<p>We use the lagrangian multipliers <span class="math">\((\alpha,\beta)\)</span> to find the extremum of the entropy <span class="math">\(S = S[p]\)</span>. The first case&nbsp;is,
</p>
<div class="math">$$S = -\int \D v \, p(v) \ln p(v) + \alpha \left( 1 - \int \D v\, p(v) \right) + \beta \left( c - \int \D v\, p(v)|v| \right)\,.$$</div>
<p>
Variation of this functional&nbsp;gives,
</p>
<div class="math">$$\frac{\delta S}{\delta p} = 0 = -1 - \ln p(v) - \alpha - \beta |v|\,.$$</div>
<p>
Variation on <span class="math">\(\alpha\)</span> gives the normalization condition, and over <span class="math">\(\beta\)</span>, gives the constraint <span class="math">\(\braket{|v|} = c\)</span>. These two conditions allow the calculation of the lagrangian multipliers. The final result&nbsp;is,
</p>
<div class="math">$$p(v) = \frac{\E^{-|v|/c}}{2c}\,.$$</div>
<p>
A similar calculation gives a gaussian distribution in the case of the constraint <span class="math">\(\braket{v^2} = c^2\)</span>:
</p>
<div class="math">$$p(v) = \frac{\E^{-v^2/2c^2}}{\sqrt{2\pi c^2}}.$$</div>
<p>Substituting these results into the expression of the entropy one can compute the information difference. For&nbsp;instance,
</p>
<div class="math">$$S = -\int \D v \, \frac{\E^{-|v|/c}}{2c} \ln \frac{\E^{-|v|/c}}{2c} = - \left. (x+1)\,\E^{-x}\right|_0^\infty - \ln(2 c)\, \left. \E^{-x}\right|_0^\infty=1 + \ln(2c) \,.$$</div>
<p>
The final result&nbsp;is,
</p>
<div class="math">$$\Delta I = \frac{1 + \ln 2/\pi}{2 \ln 2}\,.$$</div>
<h3>5</h3>
<p>A simple model of a polymer consists in a chain of bonded atoms (<span class="math">\(a\)</span> is the link length) whose successive links can make an angle of <span class="math">\(0\)</span> or <span class="math">\(\pi\)</span>, with probability <span class="math">\(1/2\)</span>, for a total length <span class="math">\(L\)</span>. </p>
<ul>
<li>Find the number <span class="math">\(\Omega\)</span> of chains of length <span class="math">\(L\)</span> as a function of the number of atoms <span class="math">\(N\)</span>. You may introduce the number of <span class="math">\(0\)</span> links <span class="math">\(N_+\)</span> and the number of <span class="math">\(\pi\)</span> links, such that <span class="math">\(L=(N_+ - N_-)a\)</span> and <span class="math">\(N = N_+ + N_-\)</span>.</li>
<li>Assume <span class="math">\(L/a \ll N\)</span> and show that <span class="math">\(\Omega(N,L)\)</span> approaches a gaussian distribution of variance <span class="math">\(\sim N\)</span> (as in a random walk). Deduce the entropy (using the Stirling&nbsp;approximation).</li>
<li>Show that the force required to maintain the chain length at a fixed value <span class="math">\(L\)</span> follows the Hooke&#8217;s law (for a well folded&nbsp;chain).</li>
<li>
<p>If <span class="math">\(L\)</span> can take any value, not necessary small with respect to <span class="math">\(Na\)</span>, use the Boltzmann weight <span class="math">\(\sim \E^{-V/T}\)</span> (<span class="math">\(V\)</span> is the potential of the external force), to demonstrate that the force&nbsp;is,</p>
<p>
<div class="math">$$f = \frac{T}{a} \mathrm{artanh}\left( \frac{L}{Na} \right)\,.$$</div>
</p>
<p>(<em>Hint</em>. Consider one&nbsp;link.)</p>
</li>
</ul>
<p>The calculation of the microcanonical entropy can be put in a form similar to the example given above about the Bernoulli process or random walk. Indeed, the entropy of a configuration with <span class="math">\(N_+\)</span> links with angle <span class="math">\(0\)</span>, and <span class="math">\(N_-\)</span> links with angle <span class="math">\(\pi\)</span> has an&nbsp;entropy,
</p>
<div class="math">$$S = \ln \frac{N!}{N_+! N_-!}$$</div>
<p>
This gives, in the relevant limit, an entropy quadratic in <span class="math">\(L\)</span>, <span class="math">\(S(T,L) \sim L^2\)</span>:
</p>
<div class="math">$$S \approx N \ln N - \frac{N+n}{2} \ln \frac{N+n}{2} -  \frac{N-n}{2} \ln \frac{N-n}{2}$$</div>
<p>
where <span class="math">\(n=L/a\)</span>; using <span class="math">\(\ln(1\pm x) \approx \pm x\)</span>, we&nbsp;obtain,
</p>
<div class="math">$$S\approx \ln 2 - \frac{L^2}{2a^2N}.$$</div>
<p>To compute the force, we can use the thermodynamic relation for the free energy <span class="math">\(F = F(T,L) = -T S(T,L) + E\)</span>,
</p>
<div class="math">$$dF = - S dT - f dL, \quad f= -f_\text{ext}\,.$$</div>
<p>
Therefore,
</p>
<div class="math">$$-\frac{\partial S}{\partial L} = \frac{\partial^2 F}{\partial L \partial T} = \frac{\partial^2 F}{\partial T \partial L} = \frac{\partial f}{\partial T}$$</div>
<p>
which leads to the&nbsp;formula
</p>
<div class="math">$$f_{\text{ext}} = T\frac{\partial S}{\partial L} \approx \frac{T}{N} L$$</div>
<p>
similar to Hooke&#8217;s law, but with a rigidity proportional to the temperature, manifestation of the entropic character of the&nbsp;force.</p>
<p>Another approach is to consider that each link is in thermal equilibrium with the applied force, independently to the other links; therefore, using the bolzmann&nbsp;weight,
</p>
<div class="math">$$p(\pm) \sim \exp \left( \pm \frac{fa}{T}  \right)$$</div>
<p>
we obtain the expected value of the total&nbsp;length,
</p>
<div class="math">$$L = Na \frac{\E^{fa/T} - \E^{-fa/T}}{\E^{fa/T} + \E^{-fa/T}} = Na \tanh(fa/T)$$</div>
<p>
This formula coincides with the previous one in the limit <span class="math">\(T \gg fa\)</span>.</p>
<h3>Random&nbsp;matrices</h3>
<h3>6</h3>
<p>We consider first, a random symmetric <span class="math">\(N \times N\)</span> matrix <span class="math">\(M\)</span>, each element <span class="math">\(i \le j\)</span> distributed according&nbsp;to
</p>
<div class="math">$$p(x) = p(M_{ij}=x) = \begin{cases}\frac{1}{2a} &amp; \text{for} \quad x\in (-a,a)\\
0 &amp; \text{otherwise}\end{cases}\,.$$</div>
<ul>
<li>
<p>Calculate the characteristic function <span class="math">\(p(k)\)</span> of the distribution <span class="math">\(p(M)\)</span> and of the distribution of the trace <span class="math">\(\mathrm{Tr}\,M\)</span>, <span class="math">\(p_T(k)\)</span>.</p>
</li>
<li>
<p>For large <span class="math">\(N\)</span> the cumulant of <span class="math">\(N\)</span> elements is <span class="math">\(N\)</span> times the cumulant of one element. Use the central limit theorem (gaussian distribution for large <span class="math">\(N\)</span>) to compute <span class="math">\(p(t)\)</span>, with <span class="math">\(t = N^{-1/2} \mathrm{Tr}\, M\)</span>; show that,
<div class="math">$$p(t) = (3/2\pi a^2)^{1/2} \E^{-3t^2/2a^2}\,.$$</div>
</p>
</li>
</ul>
<p>Consider now that <span class="math">\(M\)</span> is a <span class="math">\(2\times2\)</span> <em>gaussian</em> random matrix: each element is normally distributed with <span class="math">\(\mathcal{N}(0,1)\)</span> for the diagonal elements, and <span class="math">\(\mathcal{N}(0,1/\sqrt{2})\)</span> for the nondiagonal ones. (We denote <span class="math">\(\mathcal{N}(0,\sigma)\)</span> a normal distribution with zero mean and variance <span class="math">\(\sigma^2\)</span>.)</p>
<ul>
<li>
<p>Compute the eigenvalues <span class="math">\(\lambda_\pm\)</span> of <span class="math">\(M\)</span> and show that the probability distribution of the &#8220;spacing&#8221; <span class="math">\(s = (\lambda_+ - \lambda_-)/\Delta\)</span>, where <span class="math">\(\Delta\)</span> is such that the mean spacing is unity, is given by the <a href="/pages/rm.html">Wigner surmise</a>:
<div class="math">$$p(s) = \frac{\pi s}{2} \E^{-\pi s^2/4}\,,$$</div>
    where the mean value and normalization satisfy,
<div class="math">$$\int_0^\infty \D s\, p(s) s = \int_0^\infty \D s\, p(s) = 1\,.$$</div>
</p>
</li>
<li>
<p>Verify numerically that the Wigner surmise correctly fits the level spacing histogram of <em>large</em> random symmetric&nbsp;matrices.</p>
</li>
</ul>
<p>The characteristic function is the fourier transform of the probability&nbsp;distribution:
</p>
<div class="math">$$p_{ij}(k) = \int_{-a}^{a} \frac{\D x}{2a} \E^{-\I k x} = \frac{\sin ak}{ak}$$</div>
<p>
The probability of each random element of <span class="math">\(M\)</span> is independent of the others, therefore the characteristic function of the sum of <span class="math">\(N\)</span> random variables <span class="math">\(T = \Tr M = \sum_{i} p(M_{ii}\)</span> is the&nbsp;product,
</p>
<div class="math">$$p_T(k) = \prod_{i=1}^N p_{ii}(k) = \left(  \frac{\sin ak}{ak} \right)^N\,.$$</div>
<p>
(Note that <span class="math">\(T\)</span> is a sum of random variables, then it is itself a random&nbsp;variable.)</p>
<p>The first cumulant is determined by the mean value, which vanishes <span class="math">\(\braket{M_{ij}} = 0\)</span>:
</p>
<div class="math">$$\braket{T}_c = N \braket{M_{ij}} = 0 \,.$$</div>
<p>
The second order cumulant is simply (we use <span class="math">\(\braket{M}=0\)</span>),
</p>
<div class="math">$$\braket{T^2}_c = N \braket{M_{ij}^2} = N \int_{-a}^{a} \frac{\D x}{2a} x^2 = N \frac{a^2}{3}\,.$$</div>
<p>Let us define the reduced variable <span class="math">\(t = T/\sqrt{N}\)</span>; by the central limit theorem we&nbsp;have,
</p>
<div class="math">$$p(t) = \lim_{N \rightarrow \infty} P(T/\sqrt{N} = t) = \frac{\E^{-t^2/2 \braket{t^2}}}{\sqrt{2\pi \braket{t^2}}}\,.$$</div>
<p>
Using the value of the moment <span class="math">\(\braket{T^2}\)</span>, or <span class="math">\(\braket{t^2} = a^2/3\)</span>, we obtain the&nbsp;solution
</p>
<div class="math">$$p(t) = \sqrt{\frac{3}{2\pi a^2}} \E^{-3t^2/2 a^2}\,.$$</div>
<p>We consider now the case of a gaussian&nbsp;matrix:
</p>
<div class="math">$$M = \begin{pmatrix} a &amp; b \\ b &amp; c \end{pmatrix} \,.$$</div>
<p>
Its eigenvalues&nbsp;are,
</p>
<div class="math">$$\lambda_\pm = \frac{1}{2}\big[ \Tr M \pm \sqrt{d^2 + b^2} \big],\; d = (a-c)/2\\.$$</div>
<p>
Therefore the reduced level separation&nbsp;is,
</p>
<div class="math">$$s = \frac{\lambda_+ - \lambda_-}{\Delta} = \frac{2}{\Delta} \sqrt{d^2 + b^2}\,,$$</div>
<p>
where <span class="math">\(\Delta\)</span> is such that the probability distribution <span class="math">\(p(s)\)</span> of the level spacing&nbsp;satisfies,
</p>
<div class="math">$$\int \D s\, s p(s) = 1\,.$$</div>
<p>
This condition together with the normalisation&nbsp;condition,
</p>
<div class="math">$$\int \D s\,  p(s) = 1\,.$$</div>
<p>
means that we can determine <span class="math">\(p(s)\)</span> as a function of two unknown constants. The probability distribution of a matrix (the matrix is a random &#8220;variable&#8221;), depends only on the distribution of the random variables <span class="math">\(d\)</span> and <span class="math">\(b\)</span>:
</p>
<div class="math">$$p_M(d,b) = A \exp(- d^2 - b^2)\,$$</div>
<p>
where we used the fact that both <span class="math">\(d\)</span> and <span class="math">\(b\)</span> are gaussians of variance <span class="math">\(1/2\)</span>.&nbsp;Therefore,
</p>
<div class="math">$$p(s) = \int \D d \D b \, p_M(d,b) \delta\big(s - \sqrt{d^2 + b^2} \big)\,.$$</div>
<p>
The integral is easily computed in polar variables <span class="math">\(r^2 = d^2 + b^2\)</span>,
</p>
<div class="math">$$p(s) = C s \E^{- B s^2}\,.$$</div>
<p>
Putting this formula into the normalization and average conditions, we find <span class="math">\(C\)</span> and <span class="math">\(B\)</span>; the final result&nbsp;is,
</p>
<div class="math">$$p(s) = \frac{\pi s}{2} \E^{-\pi s^2/4}\,,$$</div>
<p>
a formula know as the Wigner surmise: it describes with a good precision the spacing distribution of the so-called <span class="caps">GOE</span>, the gaussian orthogonal ensemble of random matrices. Applications span from nuclear physics to quantum&nbsp;chaos.</p>
<h3>Notes</h3>
<div class="footnote">
<hr>
<ol>
<li id="fn:SH">
<p>Shiryaev, A. N., Probability (1996).&#160;<a class="footnote-backref" href="#fnref:SH" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:SH" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "true";

    if (true) {
        align = (screen.width < 700) ? "left" : align;
        indent = (screen.width < 700) ? "0em" : indent;
        linebreak = (screen.width < 700) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
  </div>
</article>
      </div>
    </div>

    <footer class="footer">
      <div class="container">
        <div class="row">
          <ul class="col-sm-6 list-inline">
            <li class="list-inline-item"><a href="/archives.html">Archives</a></li>
            <li class="list-inline-item"><a href="/categories.html">Categories</a></li>
              <li class="list-inline-item"><a href="/tags.html">Tags</a></li>
          </ul>
          <p class="col-sm-6 text-sm-right text-muted">
          <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a> / <a href="https://getbootstrap.com" target="_blank"><img alt="Bootstrap" src="/theme/css/bootstrap-solid.svg" style="height: 18px;"/></a> / <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License Non-Commercial 4.0" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a> CC 4.0
          </p>
        </div>
      </div>
    </footer>
  </body>

</html>